# Copilot Instructions: Arbitrary Microstructure Flow Prediction

## Project Overview

This is a **physics-informed ML project** for predicting resin flow fields (velocity and pressure) in fibrous composite microstructures using U-Net models with a sliding window technique. Not a diffusion model despite the repo name—it's computational fluid dynamics surrogate modeling.

**Key Architecture**: Two separate U-Net predictors (`VelocityPredictor` and `PressurePredictor`) trained independently, used together via `SlidingWindow` for arbitrary-sized domains.

## Critical Components

### Dual Predictor System
- **VelocityPredictor** (`src/predictor.py`): Predicts 2-channel (vx, vy) flow velocity from binary microstructure images
  - Input: 1-channel binary image (1=fluid, 0=fiber), optionally distance-transformed
  - Output: 2-channel normalized velocity field
- **PressurePredictor** (`src/predictor.py`): Predicts pressure field with physical length awareness
  - Input: 2 channels (microstructure + normalized inverse length `1/x_length`)
  - Output: 1-channel ρ-normalized pressure field
  - **Critical preprocessing**: multiplies first channel by fiber volume fraction, inverts second channel (see `pre_process`)

### Physics-Based Post-Processing
- **Velocity correction** (`src/apps.py:correct_velocity_field`): Enforces constant inlet flow rate across domain width
- **Pressure shifting** (`src/apps.py:shift_pressure_fields`): Aligns overlapping window predictions using average pressure at boundaries
- These corrections are ESSENTIAL—predictions without them (`prediction_naive`) have significantly worse physics consistency

### Sliding Window Implementation
- Fragments large rectangular microstructures into 256×256 overlapping squares
- `define_window_positions()`: Creates windows with `step_size` spacing (typically 2px) horizontally, full height divisions vertically
- `average_frames()`: Overlays predictions using weighted averaging (tracking overlap counts)
- Handles edge cases: adjusts final window position to cover uncovered domain edges

## Data Flow & Normalization

1. **Input normalization** (`src/normalizer.py:MaxNormalizer`):
   - Velocity model: no input normalization (distance transform only)
   - Pressure model: normalizes by `(1, max_length)` from `statistics.json`
2. **Output normalization**: All predictions normalized by max values during training
   - Velocity: `(max_vx, max_vy)` 
   - Pressure: `(max_p,)`
3. **Denormalization**: `.predict()` methods call `.inverse()` automatically

**Key file**: `statistics.json` in dataset root—auto-generated by `MicroFlowDataset._save_statistics()`, required for proper normalization.

## Training & Evaluation Workflows

### Training from Scratch
```bash
# Velocity model
python train.py --root-dir data/dataset --predictor-type velocity --in-channels 1 --out-channels 2

# Pressure model  
python train.py --root-dir data/dataset --predictor-type pressure --in-channels 2 --out-channels 1 --distance-transform ''
```
- Dataset auto-downloads from Zenodo if `--root-dir` empty
- Saves to timestamped folder: `trained/{date}_{name}_{predictor-type}_{hyperparams}/`
- Outputs: `model.pt`, `best_model.pt`, `log.json` (contains full config + training history)

### Model Loading Patterns
Three equivalent methods (see `src/predictor.py:Predictor`):
```python
# From local directory
predictor = Predictor.from_directory(folder, device)

# From Zenodo URL (auto-downloads & caches to `pretrained/`)
predictor = Predictor.from_url(url, device)

# Auto-detects local vs URL
predictor = Predictor.from_directory_or_url(directory_or_url, device)
```

### Evaluation
```bash
# Standard validation
python eval.py --root-dir data/dataset --split valid \
  --directory-or-url https://zenodo.org/records/17306446/files/velocity_model_base.zip?download=1

# Micrograph benchmark (uses sliding window)
python -m scripts.eval_micrograph --micrograph-dir path/to/micrographs \
  --velocity-model {url_or_path} --pressure-model {url_or_path}
```

## Project-Specific Conventions

### Tensor Shape Conventions
- **Microstructures**: `(batch, 1, height, width)` - binary with 1=fluid, 0=fiber
- **Velocity fields**: `(batch, 2, height, width)` - channels are [vx, vy]
- **Pressure fields**: `(batch, 1, height, width)` - scalar field
- **Physical dimensions**: `(batch, 3)` - [dx, dy, dz] in meters

### Configuration via `config.py`
- Uses `argparse` groups (dataset, training, optimization) processed into nested dicts
- **Attention mechanism**: String format `"start.end.heads"` (e.g., `"3..2"` = levels 3-max with 2 heads)
  - Empty string `""` = no attention
  - Parsed by `src/unet/models.py:eval_expression()`
- **Distance transform**: Bool flag that applies `scipy.ndimage.distance_transform_edt` to inputs

### Loss Functions (`src/unet/metrics.py`)
- Primary: `normalized_mae_loss` - MAE divided by mean absolute target (scale-invariant)
- Alternative: `mae_loss` - standard MAE
- **Not used in training** but available: `mass_conservation_loss` for physics constraints

## External Dependencies

- **Dataset**: Auto-downloaded from [Zenodo 16940478](https://doi.org/10.5281/zenodo.16940478) (square microstructures)
- **Pre-trained models**: [Zenodo 17306446](https://doi.org/10.5281/zenodo.17306446)
- **Micrograph benchmark**: [Zenodo 6611926](https://doi.org/10.5281/zenodo.6611926) - requires manual access request
- **Zenodo utilities** (`utils/zenodo.py`): Handles downloads, unzipping, URL detection

## Development Notes

### When Modifying Models
- **Never change normalization** without retraining—breaks pre-trained weights
- **UNet features list**: Must be power-of-2 progression for encoder/decoder symmetry
- **Attention levels**: 1-indexed in config string, converted to 0-indexed internally

### When Adding New Predictors
- Inherit from `Predictor` (abstract base in `src/predictor.py`)
- Implement: `forward()`, `predict()`, `pre_process()`, `type` property
- Override `init_normalizer()` if custom normalization needed
- Update `src/helper.py:set_model()` and `get_model()` with new type

### Dataset Handling
- `MicroFlowDataset` (`utils/dataset.py`): Handles both x-flow and y-flow simulations
  - Y-flow data rotated 90° and velocity channels swapped
- Data augmentation: Vertical flips with sign correction for vy component
- **3D dataset variant**: `MicroFlowDataset3D` uses single permeability value across slices

### Windows-Specific
- Tested on Ubuntu 22.04 LTS (development environment)
- PowerShell requires `;` for command chaining, not `&&`
- Use forward slashes or raw strings for paths in code

## Physics Context

**Darcy's Law**: `K = (Q * μ * L) / (A * ΔP)` where K=permeability, Q=flow rate, μ=viscosity (0.5 default), L=length, A=cross-section area, ΔP=pressure drop

**Flow rate calculation** (`src/physics.py:get_flow_rate`): Section-wise averaging of velocity × fluid area, not simple integration

# VAE for Microstructure Flow Field Prediction

## Project Overview
Variational Autoencoder (VAE) implementation for learning latent representations of steady-state velocity fields in 2D microstructures. The VAE compresses high-dimensional flow simulation data into compact latent spaces for computational efficiency.

## Architecture Components

### VAE Structure (`src/vae/`)
- **Encoder** (`encoder.py`): 3-stage downsampling with ResidualBlocks + AttentionBlock, outputs mean/logvar for reparameterization
  - Input: `(B, in_channels, H, W)` → Latent: `(B, latent_channels, H/4, W/4)`
  - Uses asymmetric padding `(0,1,0,1)` for stride-2 convs
- **Decoder** (`decoder.py`): Mirror architecture with 2 upsampling stages
- **VariationalAutoencoder** (`autoencoder.py`): Combined encoder-decoder with `save_model()`/`load_model()` methods
  - Saves both model weights (`vae.pt`) and training logs (`vae_log.json`) to same directory

### Training Loop (`pretrain_vae.py`)
- **Loss composition**: `reconstruction_loss + coeff * kl_loss` where `coeff=1e-3`
- **Critical preprocessing**: Divide velocity targets by `scale_factor=0.004389363341033459` before training
- **Masking**: Predictions multiplied by microstructure mask (`preds * mask`) to zero out solid regions
- Uses `normalized_mae_loss()` for reconstruction (normalizes by target magnitude per sample)

## Data Pipeline (`utils/dataset.py`)

### MicroFlowDataset
Loads 5 fields per sample from `.pt` files:
- `microstructure` (domain.pt): Binary mask of fluid/solid regions
- `velocity` (U.pt): 3-channel field, only channels `[0,1]` used (x,y components)
- `pressure` (p.pt), `dxyz` (grid spacing), `permeability`

**Dual-direction support**: Concatenates `x/` and `y/` subdirectories if both exist, rotating y-flow fields by 90° (`_rotate_y_field()`)

**Augmentation**: Horizontal flips for microstructure/pressure, with sign correction for y-velocity component

## Key Conventions

### Config Pattern
Use `argparse` parsers in `config/` directory:
```python
from config.vae import parser
args = parser.parse_args()
```
Required arg: `--in-channels`, defaults provided for `--latent-channels=4`, `--batch-size=10`, etc.

### Model Initialization
Always print trainable parameters in `__init__`:
```python
print(f'Trainable parameters: {self.trainable_params}.')
```

### Building Blocks (`src/vae/blocks.py`, `src/common.py`)
- **ResidualBlock**: GroupNorm(32) → SiLU → Conv, handles channel mismatch via 1×1 conv
- **AttentionBlock**: Uses custom `SelfAttention` from `src/common.py`, operates on flattened spatial dims
- **Padding utility**: `get_padding(kernel_size)` handles even/odd kernels

## Running Training

```bash
python pretrain_vae.py --in-channels 2 --dataset-dir /path/to/rve_5k_xy
```

Default saves to `trained/vae/`. Model automatically saves after each epoch with accumulated training logs.

## Critical Implementation Details

1. **KL divergence** expects `logvar` not variance: `kl_divergence(mu=mean, logvar=logvar)`
2. **Encoder sampling** uses reparameterization: `mu + eps * exp(0.5*logvar)`
3. **Data loading** uses 80/20 train/val split with fixed seed (2024) by default
4. **Device handling**: Auto-selects CUDA if available via `args.device`
5. **PyTorch version**: Pinned to 2.1.0 with CUDA 12.1, numpy must be <2.0

## UNet Component
Separate U-Net implementation exists in `src/unet/` (likely for direct prediction), shares common blocks from `src/common.py`.
